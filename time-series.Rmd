Time Series
========================================================

Correlation and slopes with time
================================

Time trends
-----------

```{r,echo=FALSE}
opts_chunk$set(fig.cap="")
```


* Assess existence or nature of time trends with:
  * correlation
  * regression ideas.
  
World mean temperatures
-----------------------
  

Global mean temperature every year since 1880:

```{r}
temp=read.csv("temperature.csv")
attach(temp)
plot(temperature~year,type="b")
lines(lowess(temperature~year))
```



Examining trend
---------------

* Temperatures increasing on average over time, but pattern very irregular.
* Find (Pearson) correlation with time, and test for significance:

```{r}
cor.test(temperature,year)
```

* Correlation, 0.8695, significantly different from zero. 
* CI shows how far from zero it is.

Tests for *linear* trend with *normal* data. 

Kendall correlation
-------------------

Alternative, Kendall (rank) correlation, which just tests for monotone trend (anything upward, anything downward) and is resistant to outliers:

```{r}
cor.test(temperature,year,method="kendall")
```

Kendall correlation usually closer to 0 for same data, but here P-values comparable. Trend again strongly significant.

Mann-Kendall
------------

Another way is via **Mann-Kendall**: Kendall correlation with time:

```{r}
library(Kendall)
MannKendall(temperature)
```

Answer same as previous.

Examining rate of change
------------------------

* Having seen that there *is* a change, question is "how fast is it?"
* Examine slopes:

  * regular regression slope, if you believe straight-line regression
  * Theil-Sen slope: resistant to outliers, based on medians
  
Ordinary regression against time
--------------------------------


```{r}
temp.lm=lm(temperature~year)
summary(temp.lm)
```

Slope about 0.006 degrees per year (about `r round(0.005863*130,1)` degrees over course of data).

Theil-Sen slope
---------------

Two ways, first more or less like regression:

```{r}
library(zyp)
temp.zs=zyp.sen(temperature~year,data=temp)
temp.zs
```

Theil-Sen slope, the second way
-------------------------------

Don't need to supply `year`, but lots of output:

```{r}
zyp.trend.vector(temperature)
detach(temp)
```

* `trend`: Theil-Sen slope per unit time (year, here)
* `lbound, ubound`: confidence interval (95% by default) for slope
* `trendp`: slope over entire time period
* `linear`: regression slope on same data

Other things confuse me!

Conclusions
-----------

* Linear regression slope is 0.005863
* Theil-Sen slope is 0.005676
* Very close.
* Pearson correlation is 0.8675
* Kendall correlation is 0.6993
* Kendall correlation smaller, but P-value equally significant (usually the case)



Actual time series
==================

The Kings of England
--------------------

* Age at death of Kings and Queens of England since William the Conqueror (1066):

```{r}
kings=read.table("kings.txt",header=F)
kings.ts=ts(kings)
```

Data in one long column `V1`, so `kings` is data frame with one column. Turn into `ts` time series object.

```{r}
kings.ts
```


Plotting a time series
----------------------

Plotting gives time plot:

```{r,"Kings-Time-Series"}
plot(kings.ts)
lines(lowess(kings.ts))
```

Comments
--------

* "Time" here is order of monarch from William the Conqueror (1st) to George VI (last).

* Looks to be slightly increasing trend of age-at-death

* but lots of irregularity.


Stationarity
------------

A time series is **stationary** if:

* mean is constant over time
* variability constant over time and not changing with mean.

Kings time series seems to have:

* non-constant mean
* but constant variability
* not stationary.

Usual fix for non-stationarity is *differencing*: new series 2nd - 1st, 3rd - 2nd etc.

In R, `diff`:

```{r}
kings.diff.ts=diff(kings.ts)
```

Did differencing fix stationarity?
----------------------------------

Looks stationary now:

```{r, "Differenced-Kings-Series"}
plot(kings.diff.ts,main="Kings series, differenced")
lines(lowess(kings.diff.ts))
```



Births per month in New York City
---------------------------------

from January 1946 to December 1959:

```{r}
ny=read.table("nybirths.txt",header=F)
ny.ts=ts(ny,freq=12,start=c(1946,1))
```

Note extras on `ts`:

* Time period is 1 year
* 12 observations per year (monthly) in `freq`
* First observation is 1st month of 1946 in `start`

Printing formats nicely
-----------------------

```{r}
ny.ts
```

Time plot
---------

* Time plot shows extra pattern:

```{r,fig.cap=""}
plot(ny.ts)
```

Comments on time plot
---------------------

* steady increase
* repeating pattern each year (seasonal component).
* Not stationary.

Differencing the New York births
--------------------------------

Does differencing help here?

```{r}
ny.diff.ts=diff(ny.ts)
plot(ny.diff.ts)
lines(lowess(ny.diff.ts))
```

Looks stationary, but some regular spikes.

Decomposing a seasonal time series
----------------------------------

Observations for NY births were every month. Are things the same every year?

A visual (using original data):

```{r}
ny.d=decompose(ny.ts)
plot(ny.d)
```

Decomposition bits
-------------------

Shows:

* original series
* just the trend, going steadily up (except at the start)
* a "seasonal" part: something that repeats every year
* random: what is left over ("residuals")

The seasonal part
-----------------

Fitted seasonal part is same every year, births lowest in February and highest in July:


```{r}
ny.d$seasonal
```


```{r, echo=FALSE}
set.seed(457299)
```

Time series basics
==================

White noise
-----------

Independent random normal. Knowing one value tells you nothing about the next. "Random" process.


```{r,"White-Noise"}
wn=rnorm(100)
wn.ts=ts(wn)
plot(wn.ts)
```




Lagging a time series
---------------------

This means moving a time series one (or more) steps back in time:

```{r}
x=rnorm(5)
x.1=c(NA,x)
x.0=c(x,NA)
cbind(x.0,x.1)
```

Have to glue `NA` onto **both** ends to keep series same length.

Lagging white noise
-------------------

```{r}
wn.1=c(NA,wn)
wn.0=c(wn,NA)
plot(wn.1,wn.0)
cor(wn.1,wn.0,use="c")
```

Correlation with lagged series
------------------------------

If you know about white noise at one time point, you know *nothing* about it at the next. This is shown by the scatterplot and the correlation. 


On the other hand, this:

```{r}
kings.1=c(NA,kings.ts)
kings.0=c(kings.ts,NA)
cor(kings.1,kings.0,use="c")
```

If one value larger, the next value (a bit) more likely to be larger. 

Plot of series vs. lagged series for kings data
-----------------------------------------------

```{r, "Series-lagged-kings"}
plot(kings.1,kings.0)
```

Two steps back:
---------------


```{r}
# one step back
cor(kings.1,kings.0,use="c")
# one step plus one more
kings.2=c(NA,kings.1)
kings.0=c(kings.0,NA)
cor(kings.0,kings.2,use="c")
```

Still a correlation two steps back, but smaller.

Autocorrelation
---------------

Correlation of time series with *itself* one, two,... time steps back is useful idea, called **autocorrelation**. Make a plot of it with `acf`.

White noise:

```{r}
acf(wn.ts)
```

No autocorrelations beyond chance, anywhere. Ignore 0.

Autocorrelations work best on *stationary* series.

Kings, differenced
--------------------

```{r}
acf(kings.diff.ts)
```

Comments on autocorrelations of kings series
--------------------------------------------

Negative autocorrelation at lag 1, nothing beyond that. 

* If one value of differenced series positive, next one most likely negative.
* If one king lives longer than predecessor, next one likely lives shorter.

NY births, differenced
------------------------

```{r}
acf(ny.diff.ts,main="NY births differenced")
```

Comments
---------

Lots of stuff:

* large positive autocorrelation at 1.0 years (July one year like July last year)
* large negative autocorrelation at 1 month.
* smallish but significant negative autocorrelation at 0.5 year = 6 months.
* Other stuff -- complicated.

Souvenir sales
--------------

Monthly sales for a beach souvenir shop in Queensland, Australia:

```{r}
souv=read.table("souvenir.txt",header=F)
souv.ts=ts(souv,frequency=12,start=1987)
souv.ts
```

Plot of souvenir sales
----------------------

```{r}
plot(souv.ts)
```

Comments
---------

Several problems:

* Mean goes up over time
* Variability gets larger as mean gets larger
* Not stationary

Problem-fixing
--------------

Fix non-constant variability first by taking logs:

```{r}
souv.log.ts=log(souv.ts)
plot(souv.log.ts)
```

Differencing
------------

Mean still not constant, so try taking differences:

```{r}
souv.log.diff.ts=diff(souv.log.ts)
plot(souv.log.diff.ts)
```

Comments
--------

* Now stationary
* but clear seasonal effect.

Decomposing to see the seasonal effect
--------------------------------------

```{r}
souv.d=decompose(souv.log.diff.ts)
plot(souv.d)
```

Comments
--------

**Big** drop in one month's differences. Look at seasonal component to see which:

```{r}
souv.d$seasonal
```


Autocorrelations
----------------

```{r}
acf(souv.log.diff.ts)
```

* Big positive autocorrelation at 1 year (strong seasonal effect)
* Small negative autocorrelation at 1 and 2 months.


Moving average
--------------

A particular type of time series called a **moving average** or MA process captures idea of autocorrelations at a few lags but not at others.

Here's generation of MA(1) process, with autocorrelation at lag 1 but not otherwise:

```{r}
e=rnorm(100)
y=numeric(0)
y[1]=0
beta=1
for (i in 2:100)
{
  y[i]=e[i]+beta*e[i-1]
}
y
```

Comments
--------

* `e` contains independent "random shocks". 
* Start process at 0. 
* Then, each value of the time series has that time's random shock, plus a multiple of the last time's random shock. 
* `y[i]` has shock in common with `y[i-1]`; should be a lag 1 autocorrelation. 
* But `y[i]` has no shock in common with `y[i-2]`, so no lag 2 autocorrelation (or beyond).


ACF for MA(1) process
---------------------

```{r}
acf(y)
```

As promised, everything beyond lag 1 is just chance.


AR process
----------

Another kind of time series is AR process, where each value depends on previous one, like this:

```{r}
e=rnorm(100)
x=numeric(0)
x[1]=0
alpha=0.7
for (i in 2:100)
{
  x[i]=alpha*x[i-1]+e[i]
}
x
```

Comments
--------

* Each random shock now only used for its own value of `x`
* but `x[i]` also depends on previous value `x[i-1]`
* so correlated with previous value
* *but* `x[i]` also contains multiple of `x[i-2]` and previous x's
* so all x's correlated, but autocorrelation dying away.

ACF for AR(1) series
--------------------

```{r}
acf(x)
```

Partial autocorrelation function
--------------------------------

This cuts off for an AR series:

```{r}
pacf(x)
```

The lag-2 autocorrelation should not be significant, and is only just.

PACF for an MA series
---------------------

Decays slowly for an MA series:

```{r}
pacf(y)
```

The old way of doing time series analysis
-----------------------------------------

Starting from a series with constant variability (eg. transform first to get it, as for souvenirs):

* Assess stationarity.
* If not stationary, take differences as many times as needed until it is.
* Look at ACF, see if it dies off. If it does, you have MA series.
* Look at PACF, see if that dies off. If it does, have AR series.
* If neither dies off, probably have a mixed "ARMA" series.
* Fit coefficients (like regression slopes).
* Do forecasts.


The new way of doing time series analysis (in R)
------------------------------------------------

* Transform series if needed to get constant variability
* Use package `forecast`.
* Use function `auto.arima` to estimate what kind of series best fits data.
* Use `forecast` to see what will happen in future.

Anatomy of `auto.arima` output
------------------------------

```{r}
library(forecast)
auto.arima(y)
```

* ARIMA part tells you what kind of series you are estimated to have:
  * first number (first 0) is AR (autoregressive) part
  * second number (second 0) is amount of differencing here
  * third number (1) is MA (moving average) part
  
* Below that, coefficients (with SEs)
* AICc is measure of fit (lower better)

What other models were possible?
--------------------------------

Run `auto.arima` with `trace=T`:

```{r}
auto.arima(y,trace=T)
```

Also possible were MA(2) and ARMA(1,1), both with AICc=289.5.


Doing it all the new way
====================

White noise
-----------

```{r}
wn.aa=auto.arima(wn.ts)
wn.aa
```

Best fit *is* white noise (no AR, no MA, no differencing). 

Forecasts
---------

```{r}
forecast(wn.aa)
```

Forecasts all 0, since the past doesn't help to predict future.


MA(1)
-----

```{r}
y.aa=auto.arima(y)
y.aa
y.f=forecast(y.aa)
```

Plotting the forecasts for MA(1)
--------------------------------

```{r}
plot(y.f)
```


AR(1)
-----

```{r}
x.aa=auto.arima(x)
x.aa
```

Oops!
-----

Got it wrong! Fit right AR(1) model:

```{r}
x.arima=arima(x,order=c(1,0,0))
x.arima
```

Forecasts for `x`
-----------------

```{r}
plot(forecast(x.arima))

```

Comparing wrong model
---------------------

```{r}
plot(forecast(x.aa))
```


Kings
-----

```{r}
kings.aa=auto.arima(kings.ts)
kings.aa
```

Kings forecasts
---------------

```{r}
kings.f=forecast(kings.aa)
kings.f
```

Kings forecasts, plotted
------------------------

```{r}
plot(kings.f)
```




NY births
---------

```{r}
ny.aa=auto.arima(ny.ts)
ny.aa
ny.f=forecast(ny.aa,h=36)
```

Going 36 time periods (3 years) into future.

NY births forecasts
-------------------

Not *quite* same every year:

```{r}
ny.f
```



Plotting the forecasts
----------------------

```{r}
plot(ny.f)
```


Log-souvenir sales
------------------

```{r}
souv.aa=auto.arima(souv.log.ts)
souv.aa
souv.f=forecast(souv.aa,h=27)
```

The forecasts
-------------

```{r}
plot(souv.ts)
plot(diff(souv.ts))
```


Differenced series showed low value for January (large drop). December highest, Jan and Feb lowest:

```{r}
souv.f
souv.f$mean
exp(souv.f$mean)
print.default(souv.f)
```

Plotting the forecasts
----------------------

```{r}
plot(souv.f)
```


Global mean temperatures, revisited
-----------------------------------

```{r}
attach(temp)
temp.ts=ts(temperature,start=1880)
temp.aa=auto.arima(temp.ts)
temp.aa
```

Forecasts
---------

```{r}
temp.f=forecast(temp.aa)
plot(temp.f)
```

