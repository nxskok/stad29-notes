\section{Discriminant analysis}
\frame{\sectionpage}



\begin{frame}[fragile]{Discriminant analysis}

  \begin{itemize}
  \item ANOVA and MANOVA: predict a (counted/measured) response from group membership.
  \item Discriminant analysis: predict group membership based on counted/measured variables.
  \item Covers same ground as logistic regression (and its variations), but emphasis on classifying observed data into correct groups.
  \item Does so by searching for linear combination of original variables that best separates data into groups (canonical variables).
  \item Assumption here that groups are known (for data we have). If trying to ``best separate'' data into unknown groups, see {\em cluster analysis}.
  \item Examples: revisit seed yield and weight data, peanut data,
    professions/activities data; remote-sensing data.
  \end{itemize}

\end{frame}


\begin{frame}[fragile]{Packages}

  
<<size="small">>=
library(MASS)
library(tidyverse)
library(ggrepel)
library(ggbiplot)
@   

\texttt{ggrepel} allows labelling points on a plot so they don't
overwrite each other.
\end{frame}

\begin{frame}[fragile]{About \texttt{select}}
  
  \begin{itemize}
  \item Both \texttt{dplyr} (in \texttt{tidyverse}) and \texttt{MASS}
    have a function called \texttt{select}, and \emph{they do
      different things}.
  \item How do you know which \texttt{select} is going to get called? 
  \item With \texttt{library}, the one loaded \emph{last} is visible,
    and others are not.
  \item Thus we can access the \texttt{select} in \texttt{dplyr} but
    not the one in \texttt{MASS}. If we wanted that one, we'd have to
    say \texttt{MASS::select}.
  \item This is why I loaded \texttt{MASS} before
    \texttt{tidyverse}. If I had done it the other way around, the
    \texttt{tidyverse} \texttt{select}, which I want to use, would have
    been the invisible one.  
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Example 1: seed yields and weights}


<<size="small",message=F>>=
my_url="http://www.utsc.utoronto.ca/~butler/d29/manova1.txt"
hilo=read_delim(my_url," ")
g=ggplot(hilo,aes(x=yield,y=weight,
  colour=fertilizer))+geom_point(size=4)
@ 

\begin{minipage}[t]{0.6\linewidth}
<<berzani, fig.height=4>>=
g
@   
\end{minipage}
\begin{minipage}[t]{0.38\linewidth}
  \vspace{0.1\textheight}
  
  Recall data from MANOVA: needed a multivariate analysis to find
  difference in seed yield and weight based on whether they were high
  or low fertilizer.
\end{minipage}

  
\end{frame}



\begin{frame}[fragile]{Basic discriminant analysis}

<<>>=
hilo.1=lda(fertilizer~yield+weight,data=hilo)
@ 

\begin{itemize}
\item Uses \texttt{lda} from package MASS.
\item ``Predicting'' group membership from measured variables.
\end{itemize}

\end{frame}

\begin{frame}[fragile]{Output}

  
<<size="small">>=
hilo.1
@ 

\end{frame}

\begin{frame}[fragile]{Things to take from output}
  
  \begin{itemize}
  \item Group means: high-fertilizer plants have (slightly) higher
    mean yield and weight than low-fertilizer plants.
  \item ``Coefficients of linear discriminants'': \texttt{LD1,
      LD2,}\ldots are scores constructed from observed variables that
    best separate the groups.
    \begin{itemize}
    \item For any plant, get LD1 score by taking $-0.76$ times yield
      plus $-1.25$ times weight, add up, standardize.
    \item Understand by pretending all variables standardized (mean 0,
      $+$ above mean, $-$ below mean). If yield and weight high (above
      average), contribute a $+$ to LD1 score, so LD1
      \emph{negative}. If yield and weight low (think $-$), LD1 score
      \emph{positive}.
    \item High-fertilizer plants have higher yield and weight, thus
      negative LD1 score. Low-fertilizer plants have low yield and
      weight, thus positive LD1 score.
    \item One LD1 score for each observation. Plot with actual groups.
    \end{itemize}
  \end{itemize}
  
\end{frame}

\begin{frame}[fragile]{How many linear discriminants?}
    
    \begin{itemize}
    \item Number of variables
    \item Number of groups \emph{minus 1}
    \item Smaller of these
    \item Seed yield and weight: 2 variables, 2 groups,
      $\min(2,2-1)=1$. 
    \end{itemize}
  
\end{frame}

\begin{frame}[fragile]{Getting LD scores}
  
Feed output from LDA into \texttt{predict}:

<<>>=
hilo.pred=predict(hilo.1)
@ 

Component $x$ contains LD score(s), here in descending order:

<<size="small">>=
d = cbind(hilo,hilo.pred$x) %>% arrange(desc(LD1))
d
@ 

High fertilizer have yield and weight high, negative LD1 scores.
  
\end{frame}

\begin{frame}[fragile]{Plotting LD1 scores}
  
  With one LD score, plot against (true) groups, eg. boxplot:
  
<<fig.height=3>>=
ggplot(d,aes(x=fertilizer,y=LD1))+geom_boxplot()
@   
  
\end{frame}

\begin{frame}[fragile]{Potentially misleading}
  
  \begin{itemize}
  \item These are like regression slopes:
    
<<size="small">>=
hilo.1$scaling
@     

\item Reflect change in LD1 score for 1-unit change in variables.
\item But one-unit change in variables might not be comparable:
  
<<size="footnotesize">>=
summary(hilo)
@   

\item Here, IQRs \emph{identical}, so 1-unit change in each variable
  means same thing.
  \end{itemize}
  
\end{frame}

\begin{frame}[fragile]{What else is in \texttt{hilo.pred}?}
  
<<size="small">>=
names(hilo.pred)
@     

\begin{itemize}
\item \texttt{class}: predicted fertilizer level (based on values of
  \texttt{yield} and \texttt{weight}).
\item \texttt{posterior}: predicted probability of being low or high
  fertilizer given \texttt{yield} and \texttt{weight}.
\end{itemize} 
    
\end{frame}

\begin{frame}[fragile]{Predictions and predicted groups}
  
\ldots based on \texttt{yield} and \texttt{weight}:

<<size="small">>=
cbind(hilo,predicted=hilo.pred$class)
table(obs=hilo$fertilizer,pred=hilo.pred$class)
@ 

 
\end{frame}

\begin{frame}[fragile]{Understanding the predicted groups}
  
  \begin{itemize}
  \item Each predicted fertilizer level is exactly same as observed
    one (perfect prediction).
  \item Table shows no errors: all values on top-left to bottom-right
    diagonal. 
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Posterior probabilities}
  
  show how clear-cut the classification decisions were:
  
<<size="small">>=
pp=round(hilo.pred$posterior,4)
d=cbind(hilo,hilo.pred$x,pp)
d
@ 
%$
Only obs.\ 7 has any doubt: \texttt{yield} low for a high-fertilizer,
but high \texttt{weight} makes up for it.
  
\end{frame}




% 
%\begin{frame}[fragile]{Contour plot of \texttt{LD1}}
%
%First, get some new yield and weight values for prediction. Then
%predict \texttt{LD1} for them:
%  
%<<>>=
%yy=seq(29,38,0.5)
%ww=seq(10,14,0.5)
%hilo.new=expand.grid(yield=yy,weight=ww)
%hilo.pred=predict(hilo.lda,hilo.new)
%@ 
%
%Then: plot original data, and overlay contours showing value of
%\texttt{LD1} for each \texttt{yield} and \texttt{weight} (over):
%  
%\end{frame}
%
%\begin{frame}[fragile]{Contour plot}
%
%  \begin{minipage}[t]{0.7\linewidth}
%    
%<<santini,fig.height=5>>=
%plot(yield,weight,col=fno,pch=fno)
%z=matrix(hilo.pred$x,length(yy),
%  length(ww),byrow=F)
%contour(yy,ww,z,add=T)
%@   
%  \end{minipage}
%  \begin{minipage}[t]{0.25\linewidth}
%    \begin{itemize}
%    \item \texttt{LD1} $<0$: top right
%    \item \texttt{LD1} $>0$: bottom left
%    \item \texttt{LD1=0}: boundary between high and low
%    \end{itemize}
%  \end{minipage}
%  
%<<echo=FALSE>>=
%detach(hilo)
%@   
%  
%\end{frame}

\begin{frame}[fragile]{Example 2: the peanuts}

<<message=F,size="footnotesize">>=
my_url="http://www.utsc.utoronto.ca/~butler/d29/peanuts.txt"
peanuts=read_delim("peanuts.txt"," ")
peanuts 
@ 

Recall: \texttt{location} and \texttt{variety} both significant in
MANOVA. Make combo of them (over):

  
\end{frame}

\begin{frame}[fragile]{Location-variety combos}
  
<<combos,size="small">>=
peanuts %>% unite(combo,c(variety,location)) -> 
  peanuts.combo
peanuts.combo
@ %def 

  
\end{frame}

\begin{frame}[fragile]{Discriminant analysis}
  
<<size="small">>=
peanuts.1=lda(combo~y+smk+w,data=peanuts.combo)
peanuts.1$scaling
peanuts.1$svd
@   

\begin{itemize}
\item Now 3 LDs (3 variables, 6 groups, $\min(3,6-1)=3$).
\item First: relationship of LDs to original variables. Look for
  coeffs far from zero: here,
  \begin{itemize}
  \item   high \texttt{LD1} mainly high \texttt{w}
  or low \texttt{y}.
\item high \texttt{LD2} mainly high \texttt{w}.
  \end{itemize}
\item \texttt{svd} values show relative importance of LDs:
  \texttt{LD1} much more important than \texttt{LD2}.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Group means by variable}
  
<<>>=
peanuts.1$means
@   

%$
\begin{itemize}
\item \texttt{5\_2} clearly smallest on \texttt{y}, \texttt{smk}, near
  smallest on \texttt{w}
\item \texttt{8\_2} clearly biggest on \texttt{smk}, \texttt{w}, also
  largest on \texttt{y}
\item \texttt{8\_1} large on \texttt{w}, small on \texttt{y}.
\item \texttt{scaling} links LDs with original variables,
  \texttt{means} links original variables with groups.
\item Implies: link between groups and LDs.
\end{itemize}
  
\end{frame}


\begin{frame}[fragile]{The predictions and misclassification}
  
<<>>=
peanuts.pred=predict(peanuts.1)
table(obs=peanuts.combo$combo,
      pred=peanuts.pred$class)
@   
%$
Actually classified very well. Only one \texttt{6\_2} classified as a
\texttt{5\_1}, rest all correct.
  
\end{frame}

\begin{frame}[fragile]{Posterior probabilities}

<<size="footnotesize">>=
pp=round(peanuts.pred$posterior,2)
peanuts.combo %>% select(-c(y,smk,w)) %>%
   cbind(.,pred=peanuts.pred$class,pp)
@   

\emph{Some} doubt about which combo each plant belongs in, but not too
much. The one misclassified plant was a close call.

%$
\end{frame}

\begin{frame}[fragile]{Discriminant scores, again}
  
  \begin{itemize}
  \item How are discriminant scores related to original variables?
  \item Construct data frame with original data and discriminant
    scores side by side:
<<size="small">>=
peanuts.1$scaling
lds=round(peanuts.pred$x,2)
mm=with(peanuts.combo,
  data.frame(combo,y,smk,w,lds))
@   
\item LD1 positive if \texttt{w} large and/or \texttt{y} small.
\item LD2 positive if \texttt{w} large.    
  \item But, what if \texttt{y, smk, w} differ in spread?
    
  \end{itemize}

\end{frame}

\begin{frame}[fragile]{Discriminant scores for data}

<<size="footnotesize">>=
mm
@   
  
  \begin{itemize}
\item Obs.\ 5 and 6 have most negative \texttt{LD1}: large \texttt{y},
  small \texttt{w}.
\item Obs.\ 4 has most negative \texttt{LD2}: small \texttt{w}.
\end{itemize}

\end{frame}


\begin{frame}[fragile]{Predict typical LD1 scores}
  
  First and third quartiles for three response variables (reading down):
  
<<>>=
quartiles = peanuts %>% select(y:w) %>%
  map_df(quantile, c(0.25,0.75))
quartiles
new=with(quartiles,crossing(y,smk,w))
@   
  
\end{frame}

\begin{frame}[fragile]{The combinations}
  
<<>>=
new
pp=predict(peanuts.1,new)
@   
  
\end{frame}

\begin{frame}[fragile]{Predicted typical LD1 scores}
  
<<size="footnotesize">>=
cbind(new,pp$x) %>% arrange(LD1)
@   

\begin{itemize}
\item Very negative LD1 score with large \texttt{y} and small
  \texttt{w}
\item \texttt{smk} doesn't contribute much to LD1
\item Very positive LD1 score with small \texttt{y} and large
  \texttt{w}.
\item Same as we saw from Coefficients of Linear Discriminants.
\end{itemize}
  
\end{frame}


  
\begin{frame}[fragile]{Plot LD1 vs.\ LD2, labelling by combo}
  
<<fig.height=3.5>>=
g=ggplot(mm,aes(x=LD1,y=LD2,colour=combo,
  label=combo))+geom_point()+
  geom_text_repel()+guides(colour=F) ; g
@   
  
\end{frame}

\begin{frame}[fragile]{``Bi-plot'' from \texttt{ggbiplot}}
  
% <<echo=F,message=F>>=
% library(plyr)
% library(tidyverse)
% library(ggbiplot)
% @   
% 
% <<eval=F>>=
% library(ggbiplot)
% @ 
% 
  
<<fig.height=3.5>>=
ggbiplot(peanuts.1,
  groups=factor(peanuts.combo$combo))
@ 

%$ %$ %$
  
\end{frame}

\begin{frame}[fragile]{Installing \texttt{ggbiplot}}
  
  \begin{itemize}
  \item \texttt{ggbiplot} not on CRAN, so usual
    \texttt{install.packages} will not work.
  \item Install package \texttt{devtools} first (once):
    
<<eval=F>>=
install.packages("devtools")
@     
  \item Then install \texttt{ggbiplot} (once):
<<eval=F>>=
library(devtools)
install_github("vqv/ggbiplot")
@     
  \end{itemize}
  
\end{frame}


\begin{frame}[fragile]{Cross-validation}
  
  \begin{itemize}
  \item So far, have predicted group membership from same data used to
    form the groups --- dishonest!
  \item Better: \emph{cross-validation}: form groups from all
    observations \emph{except one}, then predict group membership for
    that left-out observation.
  \item No longer cheating!
  \item Illustrate with peanuts data again.
  \end{itemize}
  
\end{frame}

\begin{frame}[fragile]{Misclassifications}
  \begin{itemize}
  \item Fitting and prediction all in one go:
  
<<>>=
peanuts.cv=lda(combo~y+smk+w,
  data=peanuts.combo,CV=T)
table(obs=peanuts.combo$combo,
      pred=peanuts.cv$class)
@   

\item Some more misclassification this time.
  \end{itemize}

\end{frame}

\begin{frame}[fragile]{Repeat of LD plot}
 
<<graziani,fig.height=3.5>>=
g
@   
  
\end{frame}

\begin{frame}[fragile]{Posterior probabilities}
  
<<size="footnotesize">>=
pp=round(peanuts.cv$posterior,3)
data.frame(obs=peanuts.combo$combo,
           pred=peanuts.cv$class,pp)
@   
  
\end{frame}

\begin{frame}[fragile]{Why more misclassification?}
  
  \begin{itemize}
  \item When predicting group membership for one observation, only
    uses the \emph{other one} in that group.
  \item So if two in a pair are far apart, or if two groups overlap,
    great potential for misclassification.
  \item Groups \texttt{5\_1} and \texttt{6\_2} overlap.
  \item \texttt{5\_2} closest to \texttt{8\_1}s looks more like an
    \texttt{8\_1} than a \texttt{5\_2} (other one far away).
  \item \texttt{8\_1}s relatively far apart and close to other things,
    so one appears to be a \texttt{5\_2} and the other an \texttt{8\_2}.
  \end{itemize}
  
\end{frame}


\begin{frame}[fragile]{Example 3: professions and leisure activities}

  \begin{itemize}
  \item 15 individuals from three different professions (politicians,
    administrators and belly dancers) each participate in four
    different leisure activities: reading, dancing, TV watching and
    skiing. After each activity they rate it on a 0--10 scale.
  \item Some of the data:

\begin{verbatim}
bellydancer 7 10 6 5
bellydancer 8 9 5 7
bellydancer 5 10 5 8
politician 5 5 5 6
politician 4 5 6 5
admin 4 2 2 5
admin 7 1 2 4
admin 6 3 3 3
\end{verbatim}
  \item How can we best use the scores on the activities to predict a person's profession?
  \item Or, what combination(s) of scores best separate data into profession groups?
  \end{itemize}

\end{frame}

\begin{frame}[fragile]{Discriminant analysis}

<<message=F>>=
my_url="http://www.utsc.utoronto.ca/~butler/d29/profile.txt"
active=read_delim(my_url," ")
active.1=lda(job~reading+dance+tv+ski,data=active)
active.1$svd
active.1$scaling
@   

\begin{itemize}
\item Two discriminants, first fair bit more important than second.
\item \texttt{LD1} depends (negatively) most on \texttt{dance}, a bit
  on \texttt{tv}.
\item \texttt{LD2} depends mostly on \texttt{tv}.
\end{itemize}

\end{frame}



\begin{frame}[fragile]{Misclassification}
  
<<>>=
active.pred=predict(active.1)
table(obs=active$job,pred=active.pred$class)
@   

Everyone correctly classified.
  
\end{frame}

\begin{frame}[fragile]{Plotting LDs}
  
<<fig.height=3,size="small">>=
mm=data.frame(job=active$job,active.pred$x,person=1:15)
g=ggplot(mm,aes(x=LD1,y=LD2,
    colour=job,label=job))+geom_point()+
    geom_text_repel()+guides(colour=F) ; g
@   
  
\end{frame}

\begin{frame}[fragile]{Biplot}
  
<<fig.height=4.5>>=
ggbiplot(active.1,groups=active$job)
@   
  
\end{frame}

\begin{frame}[fragile]{Comments on plot}
  
  \begin{itemize}
  \item Groups well separated: bellydancers top left, administrators
    top right, politicians lower middle.
  \item Bellydancers most negative on \texttt{LD1}: like dancing most.
  \item Administrators most positive on \texttt{LD1}: like dancing least.
  \item Politicians most negative on \texttt{LD2}: like TV-watching most.
  \end{itemize}
  
\end{frame}

\begin{frame}[fragile]{Plotting individual \texttt{person}s}
  
Make \texttt{label} be identifier of person. Now need legend:

<<fig.height=2.8>>=
ggplot(mm,aes(x=LD1,y=LD2,
    colour=job,label=person))+geom_point()+
    geom_text_repel()
@   
  
  
\end{frame}

\begin{frame}[fragile]{Posterior probabilities}

<<size="footnotesize">>=
pp=round(active.pred$posterior,3)
data.frame(obs=active$job,pred=active.pred$class,pp)
@   


Not much doubt.
\end{frame}

\begin{frame}[fragile]{Cross-validating the jobs-activities data}
  
Recall: no need for \texttt{predict}. Just pull out \texttt{class} and
make a table:  
  
<<>>=
active.cv=lda(job~reading+dance+tv+ski,
  data=active,CV=T)
table(obs=active$job,pred=active.cv$class)
@   

This time one of the bellydancers was classified as a politician.
  
\end{frame}

\begin{frame}[fragile]{and look at the posterior probabilities}
  
picking out the ones where things are not certain:

<<size="footnotesize">>=
pp=round(active.cv$posterior,3)
data.frame(obs=active$job,pred=active.cv$class,pp) %>%
  mutate(max=pmax(admin,bellydancer,politician)) %>%
  filter(max<0.9995)
@   
%$
\begin{itemize}
\item Bellydancer was ``definitely'' a politician!
\item One of the administrators might have been a politician too.
\end{itemize}
  
\end{frame}


\begin{frame}[fragile]{Why did things get misclassified?}

  \begin{minipage}[t]{0.7\linewidth}
<<nesta, fig.height=4.5, echo=F>>=
g
@       
  \end{minipage}
  \begin{minipage}[t]{0.28\linewidth}
    \begin{itemize}
    \item Go back to plot of discriminant scores:
    \item one bellydancer much closer to the politicians,
    \item one administrator a bit closer to the politicians.
    \end{itemize}
  \end{minipage}
  
  

  
\end{frame}


\begin{frame}[fragile]{Example 4: remote-sensing data}

  \begin{itemize}
  \item View 38 crops from air, measure 4 variables \verb=x1-x4=.
  \item Go back and record what each crop was.
  \item Can we use the 4 variables to distinguish crops?
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Reading in}
  
<<>>=
my_url="http://www.utsc.utoronto.ca/~butler/d29/remote-sensing.txt"
crops=read_table(my_url)
@   
  
\end{frame}


\begin{frame}[fragile]{Starting off: number of LDs}
  
<<>>=
crops.lda=lda(crop~x1+x2+x3+x4,data=crops)
crops.lda$svd
@ 

\begin{itemize}
\item 4 LDs (four variables, six groups).
\item 1st one important, maybe 2nd as well.
\end{itemize}
  
\end{frame}


\begin{frame}[fragile]{Connecting original variables and LDs}
  
<<>>=
crops.lda$means
round(crops.lda$scaling,3)
@   

\begin{itemize}
\item Links groups to original variables to LDs.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{\texttt{LD1} and \texttt{LD2}}
  
<<>>=
round(crops.lda$scaling,3)
@ 
%$
\begin{itemize}
\item \texttt{LD1} mostly \texttt{x1} (minus), so clover low on
  \texttt{LD1}, corn high.
\item \texttt{LD2} \texttt{x3} (minus), \texttt{x2} (plus), so
  sugarbeets should be high on \texttt{LD2}.
\end{itemize}

  
\end{frame}

\begin{frame}[fragile]{Predictions}
  
  \begin{itemize}
  \item Thus:
<<size="footnotesize">>=
crops.pred=predict(crops.lda)
table(obs=crops$crop,pred=crops.pred$class)
@   
\item Not very good, eg.\ only 6 of 11 \texttt{Clover} classified correctly.
\item Set up for plot:
  
<<>>=
mm=data.frame(crop=crops$crop,crops.pred$x)
@   
    
  \end{itemize}

  
\end{frame}


\begin{frame}[fragile]{Plotting the LDs}
  
<<piacentini,fig.height=3.1>>=
ggplot(mm,aes(x=LD1,y=LD2,colour=crop))+
  geom_point()
@   
  
\end{frame}

\begin{frame}[fragile]{Biplot}
  
<<fig.height=3.5>>=
ggbiplot(crops.lda,groups=crops$crop)
@   
  
\end{frame}

\begin{frame}[figure]{Comments}
  
  \begin{itemize}
  \item Corn high on LD1 (right).
  \item Clover all over the place, but mostly low on LD1 (left).
  \item Sugarbeets tend to be high on LD2.
  \item Cotton tends to be low on LD2.
  \item Very mixed up.
  \end{itemize}
  
\end{frame}

\begin{frame}[fragile]{Try removing Clover}

  \begin{itemize}

  \item the \texttt{dplyr} way:
    
<<>>=
crops %>% filter(crop!="Clover") -> crops2
crops2.lda=lda(crop~x1+x2+x3+x4,data=crops2)
@   

\item LDs for \texttt{crops2} will be different from before.
\item Concentrate on plot and posterior probs.

<<>>=
crops2.pred=predict(crops2.lda)
mm=data.frame(crop=crops2$crop,crops2.pred$x)
@   
  
\end{itemize}
  
\end{frame}


\begin{frame}[fragile]{\texttt{lda} output}
  
  
Different from before:

<<size="footnotesize">>=
crops2.lda$means
crops2.lda$svd
crops2.lda$scaling
@ %def 
  
\end{frame}

\begin{frame}[fragile]{Plot}

A bit more clustered:
  
<<nedved,fig.height=3.1>>=
ggplot(mm,aes(x=LD1,y=LD2,colour=crop))+
  geom_point()
@   

\end{frame}

\begin{frame}[fragile]{Biplot}
  
<<fig.height=3.6>>=
ggbiplot(crops2.lda,groups=crops2$crop)
@   
  
\end{frame}

\begin{frame}[fragile]{Quality of classification}
  
<<size="small">>=
table(obs=crops2$crop,pred=crops2.pred$class)
@   

Better.
  
\end{frame}

\begin{frame}[fragile]{Posterior probs, the wrong ones}
 
<<echo=F>>=
options(width=60)
@ %def 
  
  
{\footnotesize  
<<>>=
post=round(crops2.pred$posterior,3)
data.frame(obs=crops2$crop,pred=crops2.pred$class,post) %>%
  filter(obs!=pred)
@   
}

\begin{itemize}
\item These were the misclassified ones, but the posterior probability
  of being correct was not usually too low.
\item The correctly-classified ones are not very clear-cut either.

\end{itemize}
  
\end{frame}

\begin{frame}[fragile]{MANOVA}
  
Began discriminant analysis as a followup to MANOVA. Do our variables
significantly separate the crops (excluding Clover)?

<<>>=
response=with(crops2,cbind(x1,x2,x3,x4))
crops2.manova=manova(response~crop,data=crops2)
summary(crops2.manova)
@ 

Yes, at least one of the crops differs (in means) from the others. So
it is worth doing this analysis.

We did this the wrong way around, though!
  
\end{frame}

\begin{frame}[fragile]{The right way around}
  
  \begin{itemize}
  \item \emph{First}, do a MANOVA to see whether any of the groups
    differ significantly on any of the variables.
  \item \emph{If the MANOVA is significant}, do a discriminant
    analysis in the hopes of understanding how the groups are different.
  \item For remote-sensing data (without Clover):
    \begin{itemize}
    \item LD1 a fair bit more important than LD2 (definitely ignore LD3).
    \item LD1 depends mostly on \texttt{x1}, on which Cotton was high
      and Corn was low. 
    \end{itemize}

  \item Discriminant analysis in MANOVA plays the same kind of role
    that Tukey does in ANOVA.
  \end{itemize}
  

<<echo=F, warning=F>>=
pkgs = names(sessionInfo()$otherPkgs) 
pkgs=paste('package:', pkgs, sep = "")
x=lapply(pkgs, detach, character.only = TRUE, unload = TRUE)
@   

\end{frame}





