
# Discriminant analysis

## Discriminant analysis


* ANOVA and MANOVA: predict a (counted/measured) response from group membership.

* Discriminant analysis: predict group membership based on counted/measured variables.

* Covers same ground as logistic regression (and its variations), but emphasis on classifying observed data into correct groups.

* Does so by searching for linear combination of original variables that best separates data into groups (canonical variables).

* Assumption here that groups are known (for data we have). If trying to "best separate" data into unknown groups, see {\em cluster analysis}.

* Examples: revisit seed yield and weight data, peanut data,
professions/activities data; remote-sensing data.



## Packages
```{r size="small"}
library(MASS)
library(tidyverse)
library(ggrepel)
library(ggbiplot)
```

   

`ggrepel` allows labelling points on a plot so they don't
overwrite each other.


## About `select`


* Both `dplyr` (in `tidyverse`) and `MASS`
have a function called `select`, and \emph{they do
different things}.

* How do you know which `select` is going to get called? 

* With `library`, the one loaded *last* is visible,
and others are not.

* Thus we can access the `select` in `dplyr` but
not the one in `MASS`. If we wanted that one, we'd have to
say `MASS::select`.

* This is why I loaded `MASS` before
`tidyverse`. If I had done it the other way around, the
`tidyverse` `select`, which I want to use, would have
been the invisible one.  



## Example 1: seed yields and weights
```{r size="small",message=F}
my_url <- "http://www.utsc.utoronto.ca/~butler/d29/manova1.txt"
hilo <- read_delim(my_url, " ")
g <- ggplot(hilo, aes(
  x = yield, y = weight,
  colour = fertilizer
)) + geom_point(size = 4)
```

 

\begin{minipage}[t]{0.6\linewidth}
```{r berzani, fig.height=4}
g
```

   
\end{minipage}
\begin{minipage}[t]{0.38\linewidth}
\vspace{0.1\textheight}
Recall data from MANOVA: needed a multivariate analysis to find
difference in seed yield and weight based on whether they were high
or low fertilizer.
\end{minipage}

## Basic discriminant analysis

```{r }
hilo.1 <- lda(fertilizer ~ yield + weight, data = hilo)
```

 



* Uses `lda` from package MASS.

* "Predicting" group membership from measured variables.




## Output
```{r size="small"}
hilo.1
```

 



## Things to take from output


* Group means: high-fertilizer plants have (slightly) higher
mean yield and weight than low-fertilizer plants.

* "Coefficients of linear discriminants": \texttt{LD1,
LD2,}\ldots are scores constructed from observed variables that
best separate the groups.


* For any plant, get LD1 score by taking $-0.76$ times yield
plus $-1.25$ times weight, add up, standardize.

* Understand by pretending all variables standardized (mean 0,
$+$ above mean, $-$ below mean). If yield and weight high (above
average), contribute a $+$ to LD1 score, so LD1
*negative*. If yield and weight low (think $-$), LD1 score
*positive*.

* High-fertilizer plants have higher yield and weight, thus
negative LD1 score. Low-fertilizer plants have low yield and
weight, thus positive LD1 score.

* One LD1 score for each observation. Plot with actual groups.




## How many linear discriminants?


* Number of variables

* Number of groups *minus 1*

* Smaller of these

* Seed yield and weight: 2 variables, 2 groups,
$\min(2,2-1)=1$. 



## Getting LD scores
Feed output from LDA into `predict`:

```{r }
hilo.pred <- predict(hilo.1)
```

 

Component $x$ contains LD score(s), here in descending order:

```{r size="small"}
d <- cbind(hilo, hilo.pred$x) %>% arrange(desc(LD1))
d
```

 

High fertilizer have yield and weight high, negative LD1 scores.


## Plotting LD1 scores
With one LD score, plot against (true) groups, eg. boxplot:
```{r fig.height=3}
ggplot(d, aes(x = fertilizer, y = LD1)) + geom_boxplot()
```

   


## Potentially misleading


* These are like regression slopes:
```{r size="small"}
hilo.1$scaling
```

     


* Reflect change in LD1 score for 1-unit change in variables.

* But one-unit change in variables might not be comparable:
```{r size="footnotesize"}
summary(hilo)
```

   


* Here, IQRs *identical*, so 1-unit change in each variable
means same thing.



## What else is in `hilo.pred?`
```{r size="small"}
names(hilo.pred)
```

     



* `class`: predicted fertilizer level (based on values of
`yield` and `weight`).

* `posterior`: predicted probability of being low or high
fertilizer given `yield` and `weight`.
 


## Predictions and predicted groups
\ldots based on `yield` and `weight`:

```{r size="small"}
cbind(hilo, predicted = hilo.pred$class)
table(obs = hilo$fertilizer, pred = hilo.pred$class)
```

 


## Understanding the predicted groups


* Each predicted fertilizer level is exactly same as observed
one (perfect prediction).

* Table shows no errors: all values on top-left to bottom-right
diagonal. 



## Posterior probabilities
show how clear-cut the classification decisions were:
```{r size="small"}
pp <- round(hilo.pred$posterior, 4)
d <- cbind(hilo, hilo.pred$x, pp)
d
```
$
Only obs.\ 7 has any doubt: `yield` low for a high-fertilizer,
but high `weight` makes up for it.
 
## Contour plot of `LD1`

First, get some new yield and weight values for prediction. Then
predict `LD1` for them:
  
<<>>=
yy=seq(29,38,0.5)
ww=seq(10,14,0.5)
hilo.new=expand.grid(yield=yy,weight=ww)
hilo.pred=predict(hilo.lda,hilo.new)
@ 

Then: plot original data, and overlay contours showing value of
`LD1` for each `yield` and `weight` (over):
  


## Contour plot

  \begin{minipage}[t]{0.7\linewidth}
    
<<santini,fig.height=5>>=
plot(yield,weight,col=fno,pch=fno)
z=matrix(hilo.pred$x,length(yy),
  length(ww),byrow=F)
contour(yy,ww,z,add=T)
@   
  \end{minipage}
  \begin{minipage}[t]{0.25\linewidth}
    
    
* `LD1` $<0$: top right
    
* `LD1` $>0$: bottom left
    
* `LD1=0`: boundary between high and low
    
  \end{minipage}
  
<<echo=FALSE>>=
detach(hilo)
@   
  


## Example 2: the peanuts

```{r message=F,size="footnotesize"}
my_url <- "http://www.utsc.utoronto.ca/~butler/d29/peanuts.txt"
peanuts <- read_delim(my_url, " ")
peanuts
```

 

Recall: `location` and `variety` both significant in
MANOVA. Make combo of them (over):


## Location-variety combos
```{r combos,size="small"}
peanuts %>% unite(combo, c(variety, location)) ->
peanuts.combo
peanuts.combo
```
def 


## Discriminant analysis
```{r size="small"}
peanuts.1 <- lda(combo ~ y + smk + w, data = peanuts.combo)
peanuts.1$scaling
peanuts.1$svd
```

   



* Now 3 LDs (3 variables, 6 groups, $\min(3,6-1)=3$).

* First: relationship of LDs to original variables. Look for
coeffs far from zero: here,


*   high `LD1` mainly high `w`
or low `y`.

* high `LD2` mainly high `w`.


* `svd` values show relative importance of LDs:
`LD1` much more important than `LD2`.



## Group means by variable
```{r }
peanuts.1$means
```
$


* `5_2` clearly smallest on `y`, `smk`, near
smallest on `w`

* `8_2` clearly biggest on `smk`, `w`, also
largest on `y`

* `8_1` large on `w`, small on `y`.

* `scaling` links LDs with original variables,
`means` links original variables with groups.

* Implies: link between groups and LDs.


## The predictions and misclassification
```{r }
peanuts.pred <- predict(peanuts.1)
table(
  obs = peanuts.combo$combo,
  pred = peanuts.pred$class
)
```
$
Actually classified very well. Only one `6_2` classified as a
`5_1`, rest all correct.


## Posterior probabilities

```{r size="footnotesize"}
pp <- round(peanuts.pred$posterior, 2)
peanuts.combo %>%
  select(-c(y, smk, w)) %>%
  cbind(., pred = peanuts.pred$class, pp)
```

   

*Some* doubt about which combo each plant belongs in, but not too
much. The one misclassified plant was a close call.
$


## Discriminant scores, again


* How are discriminant scores related to original variables?

* Construct data frame with original data and discriminant
scores side by side:
```{r size="small"}
peanuts.1$scaling
lds <- round(peanuts.pred$x, 2)
mm <- with(
  peanuts.combo,
  data.frame(combo, y, smk, w, lds)
)
```

   

* LD1 positive if `w` large and/or `y` small.

* LD2 positive if `w` large.    

* But, what if `y, smk, w` differ in spread?




## Discriminant scores for data

```{r size="footnotesize"}
mm
```

   


* Obs.\ 5 and 6 have most negative `LD1`: large `y`,
small `w`.

* Obs.\ 4 has most negative `LD2`: small `w`.



## Predict typical LD1 scores
First and third quartiles for three response variables (reading down):
```{r }
quartiles <- peanuts %>%
  select(y:w) %>%
  map_df(quantile, c(0.25, 0.75))
quartiles
new <- with(quartiles, crossing(y, smk, w))
```

   


## The combinations
```{r }
new
pp <- predict(peanuts.1, new)
```

   


## Predicted typical LD1 scores
```{r size="footnotesize"}
cbind(new, pp$x) %>% arrange(LD1)
```

   



* Very negative LD1 score with large `y` and small
`w`

* `smk` doesn't contribute much to LD1

* Very positive LD1 score with small `y` and large
`w`.

* Same as we saw from Coefficients of Linear Discriminants.


## Plot LD1 vs.\ LD2, labelling by combo
```{r fig.height=3.5}
g <- ggplot(mm, aes(
  x = LD1, y = LD2, colour = combo,
  label = combo
)) + geom_point() +
  geom_text_repel() + guides(colour = F)
g
```

   


## "Bi-plot" from `ggbiplot`
 <<echo=F,message=F>>=
 library(plyr)
 library(tidyverse)
 library(ggbiplot)
 @   
 
 <<eval=F>>=
 library(ggbiplot)
 @ 
 
```{r fig.height=3.5}
ggbiplot(peanuts.1,
  groups = factor(peanuts.combo$combo)
)
```
$ %$ %$


## Installing `ggbiplot`


* `ggbiplot` not on CRAN, so usual
`install.packages` will not work.

* Install package `devtools` first (once):
```{r eval=F}
install.packages("devtools")
```

     

* Then install `ggbiplot` (once):
```{r eval=F}
library(devtools)
install_github("vqv/ggbiplot")
```

     


## Cross-validation


* So far, have predicted group membership from same data used to
form the groups --- dishonest!

* Better: *cross-validation*: form groups from all
observations *except one*, then predict group membership for
that left-out observation.

* No longer cheating!

* Illustrate with peanuts data again.



## Misclassifications


* Fitting and prediction all in one go:
```{r }
peanuts.cv <- lda(combo ~ y + smk + w,
  data = peanuts.combo, CV = T
)
table(
  obs = peanuts.combo$combo,
  pred = peanuts.cv$class
)
```

   


* Some more misclassification this time.




## Repeat of LD plot
```{r graziani,fig.height=3.5}
g
```

   


## Posterior probabilities
```{r size="footnotesize"}
pp <- round(peanuts.cv$posterior, 3)
data.frame(
  obs = peanuts.combo$combo,
  pred = peanuts.cv$class, pp
)
```

   


## Why more misclassification?


* When predicting group membership for one observation, only
uses the *other one* in that group.

* So if two in a pair are far apart, or if two groups overlap,
great potential for misclassification.

* Groups `5_1` and `6_2` overlap.

* `5_2` closest to `8_1`s looks more like an
`8_1` than a `5_2` (other one far away).

* `8_1`s relatively far apart and close to other things,
so one appears to be a `5_2` and the other an `8_2`.


## Example 3: professions and leisure activities


* 15 individuals from three different professions (politicians,
administrators and belly dancers) each participate in four
different leisure activities: reading, dancing, TV watching and
skiing. After each activity they rate it on a 0--10 scale.

* Some of the data:


```

bellydancer 7 10 6 5
bellydancer 8 9 5 7
bellydancer 5 10 5 8
politician 5 5 5 6
politician 4 5 6 5
admin 4 2 2 5
admin 7 1 2 4
admin 6 3 3 3

```


* How can we best use the scores on the activities to predict a person's profession?

* Or, what combination(s) of scores best separate data into profession groups?




## Discriminant analysis

```{r message=F}
my_url <- "http://www.utsc.utoronto.ca/~butler/d29/profile.txt"
active <- read_delim(my_url, " ")
active.1 <- lda(job ~ reading + dance + tv + ski, data = active)
active.1$svd
active.1$scaling
```

   



* Two discriminants, first fair bit more important than second.

* `LD1` depends (negatively) most on `dance`, a bit
on `tv`.

* `LD2` depends mostly on `tv`.



## Misclassification
```{r }
active.pred <- predict(active.1)
table(obs = active$job, pred = active.pred$class)
```

   

Everyone correctly classified.


## Plotting LDs
```{r fig.height=3,size="small"}
mm <- data.frame(job = active$job, active.pred$x, person = 1:15)
g <- ggplot(mm, aes(
  x = LD1, y = LD2,
  colour = job, label = job
)) + geom_point() +
  geom_text_repel() + guides(colour = F)
g
```

   


## Biplot
```{r fig.height=4.5}
ggbiplot(active.1, groups = active$job)
```

   


## Comments on plot


* Groups well separated: bellydancers top left, administrators
top right, politicians lower middle.

* Bellydancers most negative on `LD1`: like dancing most.

* Administrators most positive on `LD1`: like dancing least.

* Politicians most negative on `LD2`: like TV-watching most.



## Plotting individual `persons`
Make `label` be identifier of person. Now need legend:

```{r fig.height=2.8}
ggplot(mm, aes(
  x = LD1, y = LD2,
  colour = job, label = person
)) + geom_point() +
  geom_text_repel()
```

   


## Posterior probabilities

```{r size="footnotesize"}
pp <- round(active.pred$posterior, 3)
data.frame(obs = active$job, pred = active.pred$class, pp)
```

   
Not much doubt.


## Cross-validating the jobs-activities data
Recall: no need for `predict`. Just pull out `class` and
make a table:  
```{r }
active.cv <- lda(job ~ reading + dance + tv + ski,
  data = active, CV = T
)
table(obs = active$job, pred = active.cv$class)
```

   

This time one of the bellydancers was classified as a politician.


## and look at the posterior probabilities
picking out the ones where things are not certain:

```{r size="footnotesize"}
pp <- round(active.cv$posterior, 3)
data.frame(obs = active$job, pred = active.cv$class, pp) %>%
  mutate(max = pmax(admin, bellydancer, politician)) %>%
  filter(max < 0.9995)
```
$


* Bellydancer was "definitely" a politician!

* One of the administrators might have been a politician too.


## Why did things get misclassified?
\begin{minipage}[t]{0.7\linewidth}
```{r nesta, fig.height=4.5, echo=F}
g
```

       
\end{minipage}
\begin{minipage}[t]{0.28\linewidth}


* Go back to plot of discriminant scores:

* one bellydancer much closer to the politicians,

* one administrator a bit closer to the politicians.

\end{minipage}

## Example 4: remote-sensing data


* View 38 crops from air, measure 4 variables `x1-x4`.

* Go back and record what each crop was.

* Can we use the 4 variables to distinguish crops?



## Reading in
```{r }
my_url <- "http://www.utsc.utoronto.ca/~butler/d29/remote-sensing.txt"
crops <- read_table(my_url)
```

   

## Starting off: number of LDs
```{r }
crops.lda <- lda(crop ~ x1 + x2 + x3 + x4, data = crops)
crops.lda$svd
```

 



* 4 LDs (four variables, six groups).

* 1st one important, maybe 2nd as well.


## Connecting original variables and LDs
```{r }
crops.lda$means
round(crops.lda$scaling, 3)
```

   



* Links groups to original variables to LDs.



## `LD1 and texttt{LD2`}
```{r }
round(crops.lda$scaling, 3)
```
$


* `LD1` mostly `x1` (minus), so clover low on
`LD1`, corn high.

* `LD2` `x3` (minus), `x2` (plus), so
sugarbeets should be high on `LD2`.



## Predictions


* Thus:
```{r size="footnotesize"}
crops.pred <- predict(crops.lda)
table(obs = crops$crop, pred = crops.pred$class)
```

   

* Not very good, eg. only 6 of 11 `Clover` classified correctly.

* Set up for plot:
```{r }
mm <- data.frame(crop = crops$crop, crops.pred$x)
```

   


## Plotting the LDs
```{r piacentini,fig.height=3.1}
ggplot(mm, aes(x = LD1, y = LD2, colour = crop)) +
  geom_point()
```

   


## Biplot
```{r fig.height=3.5}
ggbiplot(crops.lda, groups = crops$crop)
```

   


\begin{frame}[figure]{Comments}


* Corn high on LD1 (right).

* Clover all over the place, but mostly low on LD1 (left).

* Sugarbeets tend to be high on LD2.

* Cotton tends to be low on LD2.

* Very mixed up.



## Try removing Clover


* the `dplyr` way:
```{r }
crops %>% filter(crop != "Clover") -> crops2
crops2.lda <- lda(crop ~ x1 + x2 + x3 + x4, data = crops2)
```

   


* LDs for `crops2` will be different from before.

* Concentrate on plot and posterior probs.

```{r }
crops2.pred <- predict(crops2.lda)
mm <- data.frame(crop = crops2$crop, crops2.pred$x)
```

   


## `lda output`
Different from before:

```{r size="footnotesize"}
crops2.lda$means
crops2.lda$svd
crops2.lda$scaling
```
def 


## Plot

A bit more clustered:
```{r nedved,fig.height=3.1}
ggplot(mm, aes(x = LD1, y = LD2, colour = crop)) +
  geom_point()
```

   



## Biplot
```{r fig.height=3.6}
ggbiplot(crops2.lda, groups = crops2$crop)
```

   


## Quality of classification
```{r size="small"}
table(obs = crops2$crop, pred = crops2.pred$class)
```

   

Better.


## Posterior probs, the wrong ones
```{r echo=F}
options(width = 60)
```
def 
{\footnotesize  
```{r }
post <- round(crops2.pred$posterior, 3)
data.frame(obs = crops2$crop, pred = crops2.pred$class, post) %>%
  filter(obs != pred)
```

   
}



* These were the misclassified ones, but the posterior probability
of being correct was not usually too low.

* The correctly-classified ones are not very clear-cut either.




## MANOVA
Began discriminant analysis as a followup to MANOVA. Do our variables
significantly separate the crops (excluding Clover)?

```{r }
response <- with(crops2, cbind(x1, x2, x3, x4))
crops2.manova <- manova(response ~ crop, data = crops2)
summary(crops2.manova)
```

 

Yes, at least one of the crops differs (in means) from the others. So
it is worth doing this analysis.

We did this the wrong way around, though!


## The right way around


* *First*, do a MANOVA to see whether any of the groups
differ significantly on any of the variables.

* *If the MANOVA is significant*, do a discriminant
analysis in the hopes of understanding how the groups are different.

* For remote-sensing data (without Clover):


* LD1 a fair bit more important than LD2 (definitely ignore LD3).

* LD1 depends mostly on `x1`, on which Cotton was high
and Corn was low. 


* Discriminant analysis in MANOVA plays the same kind of role
that Tukey does in ANOVA.

```{r echo=F, warning=F}
pkgs <- names(sessionInfo()$otherPkgs)
pkgs <- paste("package:", pkgs, sep = "")
x <- lapply(pkgs, detach, character.only = TRUE, unload = TRUE)
```

   



